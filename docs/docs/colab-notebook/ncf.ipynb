{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ncf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNH998EK2d6mdcMli31e2qG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GW5WAqASSYH"
      },
      "source": [
        "### Environment Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCGEV3WdSxfk",
        "outputId": "ba4a0300-1565-4a02-f645-fc0844c0541d"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\r\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1nWc6sIS22L"
      },
      "source": [
        "# Install latest release version of analytics-zoo \r\n",
        "# Installing analytics-zoo from pip will automatically install pyspark, bigdl, and their dependencies.\r\n",
        "!pip install analytics-zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR6mnxkJS3nd"
      },
      "source": [
        "# Install python dependencies\r\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fUm8BXjS-R4"
      },
      "source": [
        "### **NCF example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QosmOLUgR07w",
        "outputId": "a9db84a0-8137-40ac-f44e-006edd5d80b3"
      },
      "source": [
        "#\r\n",
        "# Copyright 2018 Analytics Zoo Authors.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "#\r\n",
        "import os\r\n",
        "import zipfile\r\n",
        "import argparse\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from bigdl.dataset import base\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from zoo.orca import init_orca_context, stop_orca_context\r\n",
        "from zoo.orca.learn.tf.estimator import Estimator\r\n",
        "from zoo.orca.data import SharedValue\r\n",
        "import zoo.orca.data.pandas\r\n",
        "\r\n",
        "SOURCE_URL = 'http://files.grouplens.org/datasets/movielens/'\r\n",
        "COLUMN_NAMES = ['user', 'item', 'label']\r\n",
        "\r\n",
        "\r\n",
        "def re_index(s):\r\n",
        "    \"\"\" for reindexing the item set. \"\"\"\r\n",
        "    i = 0\r\n",
        "    s_map = {}\r\n",
        "    for key in s:\r\n",
        "        s_map[key] = i\r\n",
        "        i += 1\r\n",
        "\r\n",
        "    return s_map\r\n",
        "\r\n",
        "\r\n",
        "def set_index(data, user_map, item_map):\r\n",
        "    def set_user_item(df, item_map, user_map):\r\n",
        "        user_list = []\r\n",
        "        item_list = []\r\n",
        "        item_map = item_map.value\r\n",
        "        user_map = user_map.value\r\n",
        "        for i in range(len(df)):\r\n",
        "            user_list.append(user_map[df['user'][i]])\r\n",
        "            item_list.append(item_map[df['item'][i]])\r\n",
        "        df['user'] = user_list\r\n",
        "        df['item'] = item_list\r\n",
        "        return df\r\n",
        "\r\n",
        "    user_map_shared_value = SharedValue(user_map)\r\n",
        "    item_map_shared_value = SharedValue(item_map)\r\n",
        "    return data.transform_shard(set_user_item, item_map_shared_value, user_map_shared_value)\r\n",
        "\r\n",
        "\r\n",
        "def load_data(data_dir):\r\n",
        "    WHOLE_DATA = 'ml-1m.zip'\r\n",
        "    local_file = base.maybe_download(WHOLE_DATA, data_dir, SOURCE_URL + WHOLE_DATA)\r\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\r\n",
        "    extracted_to = os.path.join(data_dir, \"ml-1m\")\r\n",
        "    if not os.path.exists(extracted_to):\r\n",
        "        print(\"Extracting %s to %s\" % (local_file, data_dir))\r\n",
        "        zip_ref.extractall(data_dir)\r\n",
        "        zip_ref.close()\r\n",
        "    rating_files = os.path.join(extracted_to, \"ratings.dat\")\r\n",
        "\r\n",
        "    # replace :: to : for spark 2.4 support\r\n",
        "    new_rating_files = os.path.join(extracted_to, \"ratings_new.dat\")\r\n",
        "    if not os.path.exists(new_rating_files):\r\n",
        "        fin = open(rating_files, \"rt\")\r\n",
        "        # output file to write the result to\r\n",
        "        fout = open(new_rating_files, \"wt\")\r\n",
        "        # for each line in the input file\r\n",
        "        for line in fin:\r\n",
        "            # read replace the string and write to output file\r\n",
        "            fout.write(line.replace('::', ':'))\r\n",
        "        # close input and output files\r\n",
        "        fin.close()\r\n",
        "        fout.close()\r\n",
        "\r\n",
        "    # read movive len csv to XShards of Pandas Dataframe\r\n",
        "    full_data = zoo.orca.data.pandas.read_csv(new_rating_files, sep=':', header=None,\r\n",
        "                                              names=COLUMN_NAMES, usecols=[0, 1, 2],\r\n",
        "                                              dtype={0: np.int32, 1: np.int32, 2: np.int32})\r\n",
        "\r\n",
        "    user_set = set(full_data['user'].unique())\r\n",
        "    item_set = set(full_data['item'].unique())\r\n",
        "\r\n",
        "    min_user_id = min(user_set)\r\n",
        "    max_user_id = max(user_set)\r\n",
        "    min_item_id = min(item_set)\r\n",
        "    max_item_id = max(item_set)\r\n",
        "    print(min_user_id, max_user_id, min_item_id, max_item_id)\r\n",
        "\r\n",
        "    # update label starting from 0\r\n",
        "    def update_label(df):\r\n",
        "        df['label'] = df['label'] - 1\r\n",
        "        return df\r\n",
        "\r\n",
        "    full_data = full_data.transform_shard(update_label)\r\n",
        "\r\n",
        "    # split to train/test dataset\r\n",
        "    def split_train_test(data):\r\n",
        "        # splitting the full set into train and test sets.\r\n",
        "        train, test = train_test_split(data, test_size=0.2, random_state=100)\r\n",
        "        return train, test\r\n",
        "\r\n",
        "    train_data, test_data = full_data.transform_shard(split_train_test).split()\r\n",
        "\r\n",
        "    def to_train_val_shard(df):\r\n",
        "        result = {\r\n",
        "            \"x\": (df['user'].to_numpy(), df['item'].to_numpy()),\r\n",
        "            \"y\": df['label'].to_numpy()\r\n",
        "        }\r\n",
        "        return result\r\n",
        "\r\n",
        "    train_data = train_data.transform_shard(to_train_val_shard)\r\n",
        "    test_data = test_data.transform_shard(to_train_val_shard)\r\n",
        "    return train_data, test_data, max_user_id, max_item_id\r\n",
        "\r\n",
        "\r\n",
        "class NCF(object):\r\n",
        "    def __init__(self, embed_size, user_size, item_size):\r\n",
        "        self.user = tf.placeholder(dtype=tf.int32, shape=(None,))\r\n",
        "        self.item = tf.placeholder(dtype=tf.int32, shape=(None,))\r\n",
        "        self.label = tf.placeholder(dtype=tf.int32, shape=(None,))\r\n",
        "\r\n",
        "        with tf.name_scope(\"GMF\"):\r\n",
        "            user_embed_GMF = tf.contrib.layers.embed_sequence(self.user,\r\n",
        "                                                              vocab_size=user_size + 1,\r\n",
        "                                                              embed_dim=embed_size,\r\n",
        "                                                              unique=False\r\n",
        "                                                              )\r\n",
        "            item_embed_GMF = tf.contrib.layers.embed_sequence(self.item,\r\n",
        "                                                              vocab_size=item_size + 1,\r\n",
        "                                                              embed_dim=embed_size,\r\n",
        "                                                              unique=False\r\n",
        "                                                              )\r\n",
        "            GMF = tf.multiply(user_embed_GMF, item_embed_GMF, name='GMF')\r\n",
        "\r\n",
        "        # MLP part starts\r\n",
        "        with tf.name_scope(\"MLP\"):\r\n",
        "            user_embed_MLP = tf.contrib.layers.embed_sequence(self.user,\r\n",
        "                                                              vocab_size=user_size + 1,\r\n",
        "                                                              embed_dim=embed_size,\r\n",
        "                                                              unique=False,\r\n",
        "                                                              )\r\n",
        "\r\n",
        "            item_embed_MLP = tf.contrib.layers.embed_sequence(self.item,\r\n",
        "                                                              vocab_size=item_size + 1,\r\n",
        "                                                              embed_dim=embed_size,\r\n",
        "                                                              unique=False\r\n",
        "                                                              )\r\n",
        "            interaction = tf.concat([user_embed_MLP, item_embed_MLP],\r\n",
        "                                    axis=-1, name='interaction')\r\n",
        "\r\n",
        "            layer1_MLP = tf.layers.dense(inputs=interaction,\r\n",
        "                                         units=embed_size * 2,\r\n",
        "                                         name='layer1_MLP')\r\n",
        "            layer1_MLP = tf.layers.dropout(layer1_MLP, rate=0.2)\r\n",
        "\r\n",
        "            layer2_MLP = tf.layers.dense(inputs=layer1_MLP,\r\n",
        "                                         units=embed_size,\r\n",
        "                                         name='layer2_MLP')\r\n",
        "            layer2_MLP = tf.layers.dropout(layer2_MLP, rate=0.2)\r\n",
        "\r\n",
        "            layer3_MLP = tf.layers.dense(inputs=layer2_MLP,\r\n",
        "                                         units=embed_size // 2,\r\n",
        "                                         name='layer3_MLP')\r\n",
        "            layer3_MLP = tf.layers.dropout(layer3_MLP, rate=0.2)\r\n",
        "\r\n",
        "        # Concate the two parts together\r\n",
        "        with tf.name_scope(\"concatenation\"):\r\n",
        "            concatenation = tf.concat([GMF, layer3_MLP], axis=-1,\r\n",
        "                                      name='concatenation')\r\n",
        "            self.logits = tf.layers.dense(inputs=concatenation,\r\n",
        "                                          units=5,\r\n",
        "                                          name='predict')\r\n",
        "\r\n",
        "            self.logits_softmax = tf.nn.softmax(self.logits)\r\n",
        "\r\n",
        "            self.class_number = tf.argmax(self.logits_softmax, 1)\r\n",
        "\r\n",
        "        with tf.name_scope(\"loss\"):\r\n",
        "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n",
        "                labels=self.label, logits=self.logits, name='loss'))\r\n",
        "\r\n",
        "        with tf.name_scope(\"optimzation\"):\r\n",
        "            self.optim = tf.train.AdamOptimizer(1e-3, name='Adam')\r\n",
        "            self.optimizer = self.optim.minimize(self.loss)\r\n",
        "\r\n",
        "\r\n",
        "def train(train_data, test_data, user_size, item_size):\r\n",
        "        model = NCF(opt.embedding_size, user_size, item_size)\r\n",
        "\r\n",
        "        estimator = Estimator.from_graph(\r\n",
        "            inputs=[model.user, model.item],\r\n",
        "            outputs=[model.class_number],\r\n",
        "            labels=[model.label],\r\n",
        "            loss=model.loss,\r\n",
        "            optimizer=model.optim,\r\n",
        "            model_dir=opt.model_dir,\r\n",
        "            metrics={\"loss\": model.loss})\r\n",
        "\r\n",
        "        estimator.fit(data=train_data,\r\n",
        "                      batch_size=opt.batch_size,\r\n",
        "                      epochs=opt.epochs,\r\n",
        "                      validation_data=test_data\r\n",
        "                      )\r\n",
        "\r\n",
        "        checkpoint_path = os.path.join(opt.model_dir, \"NCF.ckpt\")\r\n",
        "        estimator.save_tf_checkpoint(checkpoint_path)\r\n",
        "        estimator.sess.close()\r\n",
        "\r\n",
        "\r\n",
        "def predict(predict_data, user_size, item_size):\r\n",
        "\r\n",
        "    def to_predict(data):\r\n",
        "        del data['y']\r\n",
        "        return data\r\n",
        "\r\n",
        "    predict_data = predict_data.transform_shard(to_predict)\r\n",
        "\r\n",
        "    tf.reset_default_graph()\r\n",
        "\r\n",
        "    with tf.Session() as sess:\r\n",
        "        model = NCF(opt.embedding_size, user_size, item_size)\r\n",
        "\r\n",
        "        saver = tf.train.Saver(tf.global_variables())\r\n",
        "        checkpoint_path = os.path.join(opt.model_dir, \"NCF.ckpt\")\r\n",
        "        saver.restore(sess, checkpoint_path)\r\n",
        "\r\n",
        "        estimator = Estimator.from_graph(\r\n",
        "            inputs=[model.user, model.item],\r\n",
        "            outputs=[model.class_number],\r\n",
        "            sess=sess,\r\n",
        "            model_dir=opt.model_dir\r\n",
        "        )\r\n",
        "        predict_result = estimator.predict(predict_data)\r\n",
        "        predictions = predict_result.collect()\r\n",
        "        assert 'prediction' in predictions[0]\r\n",
        "        print(predictions[0]['prediction'])\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    parser = argparse.ArgumentParser(\r\n",
        "        description='NCF example on movie len dataset.')\r\n",
        "    parser.add_argument('--cluster_mode', type=str, default=\"local\",\r\n",
        "                        help='The mode for the Spark cluster. local or yarn.')\r\n",
        "    parser.add_argument('--data_dir', type=str, default='/tmp',\r\n",
        "                        help='the dir for downloaded data.')\r\n",
        "    parser.add_argument('--embedding_size', type=int, default=16,\r\n",
        "                        help='the size for embedding user and item.')\r\n",
        "    parser.add_argument('--model_dir', type=str, default='./',\r\n",
        "                        help='the dir for saving model.')\r\n",
        "    parser.add_argument('-b', '--batch_size', type=int, default=1280,\r\n",
        "                        help='size of mini-batch')\r\n",
        "    parser.add_argument('-e', '--epochs', type=int, default=10,\r\n",
        "                        help='The number of epochs to train the model.')\r\n",
        "\r\n",
        "    #opt = parser.parse_args()\r\n",
        "    opt = parser.parse_args(args=[])\r\n",
        "    if opt.cluster_mode == \"local\":\r\n",
        "        init_orca_context(cluster_mode=\"local\", cores=4)\r\n",
        "    elif opt.cluster_mode == \"yarn\":\r\n",
        "        init_orca_context(cluster_mode=\"yarn-client\", num_nodes=2, cores=2, driver_memory=\"6g\")\r\n",
        "\r\n",
        "    (train_data, test_data, max_user_id, max_item_id) = load_data(opt.data_dir)\r\n",
        "\r\n",
        "    train(train_data, test_data, max_user_id, max_item_id)\r\n",
        "\r\n",
        "    predict(test_data, max_user_id, max_item_id)\r\n",
        "\r\n",
        "    stop_orca_context()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing orca context\n",
            "Current pyspark location is : /usr/local/lib/python3.6/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.6/dist-packages/zoo/share/lib/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.9.0-jar-with-dependencies.jar:/usr/local/lib/python3.6/dist-packages/bigdl/share/lib/bigdl-0.12.1-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n",
            "1 6040 1 3952\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-660f3a4aaf8a>:167: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-660f3a4aaf8a>:168: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/util/tf.py:37: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/orca/learn/tf/estimator.py:354: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/orca/learn/tf/estimator.py:355: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_dataset.py:209: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_dataset.py:210: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "creating: createFakeOptimMethod\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:295: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:159: The name tf.is_numeric_tensor is deprecated. Please use tf.debugging.is_numeric_tensor instead.\n",
            "\n",
            "creating: createStatelessMetric\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:187: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:202: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:243: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:276: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tf_optimizer.py:285: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "creating: createTFTrainingHelper\n",
            "creating: createIdentityCriterion\n",
            "creating: createTFParkSampleToMiniBatch\n",
            "creating: createTFParkSampleToMiniBatch\n",
            "creating: createEstimator\n",
            "creating: createMaxEpoch\n",
            "creating: createEveryEpoch\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpc5rjafrc/model\n",
            "WARNING:tensorflow:Issue encountered when serializing list_input_0:0.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'TFNdarrayDataset' object has no attribute 'name'\n",
            "WARNING:tensorflow:Issue encountered when serializing list_input_1:0.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'TFNdarrayDataset' object has no attribute 'name'\n",
            "WARNING:tensorflow:Issue encountered when serializing list_input_0_1:0.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'TFNdarrayDataset' object has no attribute 'name'\n",
            "INFO:tensorflow:Restoring parameters from ./NCF.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/zoo/tfpark/tfnet.py:275: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /tmp/tmphur8tncz/saved_model.pb\n",
            "[2. 2. 4. ... 2. 4. 4.]\n",
            "Stopping orca context\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}